---
id: xutx0mtszok5w9q8n6d19xt
title: 01_Arxiv
desc: ''
updated: 1703046388875
created: 1699497182153
---


![图 0](assets/images/cc414f0cf3d4025af5df1a8f950208845b69deb84a4938c59342c746718d46d1.png)  


# **大模型**
## MLLM

* NExT-Chat: An LMM for Chat, Detection and Segmentation，2023年11月9日，https://arxiv.org/pdf/2311.04498.pdf
* OtterHD: A High-Resolution Multi-modality Model, 2023年11月8日，https://arxiv.org/pdf/2311.04219.pdf
* Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs，https://arxiv.org/pdf/2311.14656.pdf，2023年11月27日
* OneLLM: One Framework to Align All Modalities with Language，2023年12月7日12:37:54
* Silkie: Preference Distillation for Large Visual Language Models，2023年12月19日19:38:31
* Gemini: A Family of Highly Capable Multimodal Models，2023年12月20日12:26:27



## VLM
* Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks，2023年11月13日，https://arxiv.org/pdf/2311.06242.pdf
* Language-Assisted 3D Scene Understanding，2023年12月19日19:39:57

---


# Visual In-Context Learning
* Visual In-Context Prompting, https://arxiv.org/pdf/2311.13601.pdf, 2023年11月23日12:49:48
* IMPROV: INPAINTING-BASED MULTIMODAL PROMPTING FOR COMPUTER VISION TASKS，https://arxiv.org/pdf/2312.01771.pdf，2023年12月6日23:52:47
* Improving In-Context Learning in Diffusion Models with Visual Context-Modulated Prompts，2023年12月7日00:14:36，https://arxiv.org/abs/2312.01408
* Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning，2023年12月7日12:34:30
* 4M: Massively Multimodal Masked Modeling，2023年12月12日13:16:47
* Flexible visual prompts for in-context learning in computer vision，2023年12月12日13:16:55

---

# Diffusion

## Control
* ToddlerDiffusion: Flash Interpretable Controllable Diffusion Model, https://arxiv.org/pdf/2311.14542.pdf, 2023年11月27日
* ControlNet-XS: Designing an Efficient and Effective Architecture for Controlling Text-to-Image Diffusion Models，2023年12月12日13:17:05
* LoRA-Enhanced Distillation on Guided Diffusion Models，2023年12月13日12:43:33
* FineControlNet: Fine-level Text Control for Image Generation with Spatially Aligned Text Control Injection, 2023年12月15日10:44:24
* Guided Diffusion from Self-Supervised Diffusion Features，2023年12月15日10:51:01
* On Inference Stability for Diffusion Models, 2023年12月20日12:24:19
* 

## Efficient-Diffusion Model
* LCM-LORA: A UNIVERSAL STABLE-DIFFUSION
ACCELERATION MODULE, https://arxiv.org/pdf/2311.05556.pdf，2023年11月10日18:43:16
* WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on, 2023年12月7日12:38:09
* SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation，2023年12月11日14:00:07
* Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models，2023年12月18日20:12:50
---

# **3D** 

## 3D-aware Generation

* WILDFUSION: LEARNING 3D-AWARE LATENT DIFFUSION MODELS IN VIEW SPACE, https://arxiv.org/pdf/2311.13570.pdf，2023年11月23日12:48:32
* Enhancing Diffusion Models with 3D Perspective Geometry Constraints，2023年12月7日00:30:50
* CAD: Photorealistic 3D Generation via Adversarial Distillation, 2023年12月12日13:15:43
* Mosaic-SDF for 3D Generative Models，2023年12月15日10:47:23
* Self-supervised Learning for Enhancing Geometrical Modeling in 3D-Aware Generative Adversarial Network

## 纹理上色
* 3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models, https://arxiv.org/pdf/2311.05464.pdf，2023年11月10日18:43:19
* Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering，2023年12月19日19:40:20


## Gaussian Splatting
* GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting，https://arxiv.org/pdf/2311.14521.pdf，2023年11月27日
* GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis，2023年12月7日00:35:30
* COLMAP-Free 3D Gaussian Splatting，2023年12月13日12:42:21
* DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes，2023年12月14日15:12:11
* Text2Immersion: Generative Immersive Scene with 3D Gaussians，2023年12月15日10:46:54
* Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers，2023年12月15日10:48:24


## Mesh
* Consistent Mesh Diffusion，2023年12月7日00:30:24

---


# Image 

## Style & Appearance
* ControlStyle: Text-Driven Stylized Image Generation Using Diffusion Priors，https://arxiv.org/pdf/2311.05463.pdf，2023年11月10日18:43:29
* ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation，https://arxiv.org/abs/2312.02109，2023年12月6日
* Style Aligned Image Generation via Shared Attention，2023年12月7日00:38:03


## Editing
* Inversion-Free Image Editing with Natural Language，2023年12月11日13:59:02
* A Compact and Semantic Latent Space for Disentangled and Controllable Image Editing, 2023年12月14日15:10:14
* AdapEdit: Spatio-Temporal Guided Adaptive Editing Algorithm for Text-Based Continuity-Sensitive Image Editing，2023年12月14日15:11:46
* LIME: Localized Image Editing via Attention Regularization in Diffusion Models，2023年12月15日10:51:28
* SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing，2023年12月19日19:40:10

## Generation
* FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation，2023年12月7日12:29:17
* KANDINSKY 3.0 TECHNICAL REPORT, 2023年12月7日12:31:51
* Self-conditioned Image Generation via Generating Representations，2023年12月7日12:37:27，**KAIMING**
* Context Diffusion: In-Context Aware Image Generation，2023年12月7日12:41:59
* CONFORM: Contrast is All You Need For High-Fidelity Text-to-Image Diffusion Models，2023年12月12日13:18:26
* InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models，2023年12月12日13:19:17
* FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition，2023年12月13日12:41:58
* Learned representation-guided diffusion models for large-image generation，2023年12月13日12:42:53
* CCM: Adding Conditional Controls to Text-to-Image Consistency Models，2023年12月13日12:43:14
* Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models，2023年12月13日12:43:49
* Semantic-aware Data Augmentation for Text-to-image Synthesis，2023年12月14日15:11:57
* SpeedUpNet: A Plug-and-Play Hyper-Network for Accelerating Text-to-Image Diffusion Models，2023年12月15日10:50:17
* Semantic-Driven Initial Image Construction for Guided Image Synthesis in Diffusion Model，2023年12月15日10:50:46
* Local Conditional Controlling for Text-to-Image Diffusion Models，2023年12月15日10:51:12
* Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models，2023年12月19日19:38:06
* Rich Human Feedback for Text-to-Image Generation，2023年12月19日19:39:01

## Inpainting
* A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting，2023年12月7日12:40:45
* DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models，2023年12月10日21:36:54

## Others
* Instance-guided Cartoon Editing with a Large-scale Dataset，https://arxiv.org/abs/2312.01943，2023年12月6日
* Diffusing Colors: Image Colorization with Text Guided Diffusion，2023年12月10日21:35:56
* Fine-Tuning InstructPix2Pix for Advanced Image Colorization,2023年12月11日13:58:25
* HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models，2023年12月11日13:58:39
* Stable Rivers: A Case Study in the Application of Text-to-Image Generative Models for Earth Sciences，2023年12月14日15:12:23
* Brush Your Text: Synthesize Any Scene Text on Images via Diffusion Model，2023年12月20日12:24:51


## 定制化
* Customization Assistant for Text-to-image Generation，2023年12月7日12:33:43
* InstructBooth: Instruction-following Personalized Text-to-Image Generation，2023年12月7日12:34:14
* PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization，2023年12月12日13:17:56
* DisControlFace: Disentangled Control for Personalized Facial Image Editing，2023年12月12日13:18:03
* Concept-centric Personalization with Large-scale Diffusion Priors，2023年12月14日15:10:30
* Compositional Inversion for Stable Diffusion Models，2023年12月14日15:11:29
* Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image Diffusion Models，2023年12月20日12:24:38
---


# 3D Generation

## Text+X->3D
* Instant3D: Instant Text-to-3D Generation，https://arxiv.org/pdf/2311.08403.pdf，2023年11月16日
* Control3D: Towards Controllable Text-to-3D Generation，https://arxiv.org/pdf/2311.05461.pdf, 2023年11月10日18:43:41
* IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts，https://arxiv.org/pdf/2310.05375.pdf，2023.10.09
* Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model, 2023年11月13日，https://openreview.net/forum?id=2lDQLiH1W4（ICLR24 review 信息）
* ControlDreamer: Stylized 3D Generation with Multi-View ControlNet,https://arxiv.org/abs/2312.01129,2023年12月7日00:20:17
* Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors，2023年12月11日13:58:48
* Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior，2023年12月12日13:16:32
* DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior，2023年12月12日13:17:42
* PI3D: Efficient Text-to-3D Generation with Pseudo-Image Diffusion，2023年12月15日10:48:47
* SEEAvatar: Photorealistic Text-to-3D Avatar Generation with Constrained Geometry and Appearance，2023年12月15日10:49:20
* VolumeDiffusion: Flexible Text-to-3D Generation with Efficient Volumetric Encoder，2023年12月19日19:39:36
* Text-Image Conditioned Diffusion for Consistent Text-to-3D Generation，2023年12月20日12:26:11

## Image->3D
* HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image，2023年12月10日21:35:03
* R2Human: Real-Time 3D Human Appearance Rendering from a Single Image，2023年12月12日13:19:39
* 3DGEN: A GAN-based approach for generating novel 3D models from image data，2023年12月14日15:10:42
* Stable Score Distillation for High-Quality 3D Generation，2023年12月18日20:13:32


## 单视角合成
* One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion, https://arxiv.org/pdf/2311.07885.pdf, 2023年11月16日
* Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to 3D
Prior with Progressive Learning, https://arxiv.org/pdf/2311.13617.pdf, 2023年11月27日
* MVControl: Adding Conditional Control to Multi-view Diffusion for
Controllable Text-to-3D Generation，https://arxiv.org/pdf/2311.14494.pdf，2023年11月27日
* ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models，https://arxiv.org/abs/2312.01305，2023年12月7日00:17:11
* Free3D: Consistent Novel View Synthesis without 3D Representation，2023年12月10日
* NVS-Adapter: Plug-and-Play Novel View Synthesis from a Single Image，2023年12月13日12:43:04
* Novel View Synthesis with View-Dependent Effects from a Single Image，2023年12月14日15:11:08
* 
## 多视角重建
* DMV3D: DENOISING MULTI-VIEW DIFFUSION USING
3D LARGE RECONSTRUCTION MODEL, https://arxiv.org/pdf/2311.09217.pdf, 2023年11月16日
*  DreamComposer: Controllable 3D Object Generation via Multi-View Conditions, 2023年12月7日12:39:52


## Editing
* Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training，https://arxiv.org/abs/2312.01663，2023年12月6日23:58:57
* Mesh-Guided Neural Implicit Field Editing，2023年12月7日00:33:04
* Learning Naturally Aggregated Appearance for Efficient 3D Editing，2023年12月12日13:16:22
* SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds，2023年12月15日10:46:09
* LatentEditor: Text Driven Local Editing of 3D Scenes，2023年12月18日20:13:06
* Plasticine3D: Non-rigid 3D editting with text guidance，2023年12月19日19:39:24

## NeRF
* SANeRF-HQ: Segment Anything for NeRF in High Quality，https://arxiv.org/pdf/2312.01531.pdf，2023年12月7日00:02:40
* NeRFiller: Completing Scenes via Generative 3D Inpainting，2023年12月10日
* LAENeRF: Local Appearance Editing for Neural Radiance Fields, 2023年12月18日20:12:20
*  

--- 

# 4D

## 4D Dynamic Scene
* Animate124: Animating One Image to 4D Dynamic Scene, https://arxiv.org/pdf/2311.14603.pdf, 2023年11月27日
* Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle，2023年12月7日12:32:26
* 

---

# Video
* Decouple Content and Motion for Conditional Image-to-Video Generation,https://arxiv.org/pdf/2311.14294.pdf, 2023年11月27日
* VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence, https://arxiv.org/pdf/2312.02087.pdf，2023年12月6日

* Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models， https://arxiv.org/pdf/2312.01409.pdf，2023年12月7日00:10:19
* QPoser: Quantized Explicit Pose Prior Modeling for Controllable Pose Generation，https://arxiv.org/abs/2312.01104，2023年12月7日00:23:32
* VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models，2023年12月7日00:31:15
* F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis
* DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance，2023年12月7日12:33:57
* MotionCtrl: A Unified and Flexible Motion Controller for Video Generation，2023年12月7日12:38:35
* MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model, 2023年12月7日13:41:20
* GenDeF: Learning Generative Deformation Field for Video Generation, 2023年12月10日
* GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation，2023年12月10日
* RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models，2023年12月10日21:35:16
* Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation，2023年12月10日21:35:24
* DreamVideo: Composing Your Dream Videos with Customized Subject and Motion，2023年12月10日21:35:38
* Towards 4D Human Video Stylization，2023年12月10日21:36:05
* MTVG : Multi-text Video Generation with Text-to-Video Models，2023年12月10日21:36:12
* AVID: Any-Length Video Inpainting with Diffusion Model，2023年12月10日21:36:20
* AnimateZero: Video Diffusion Models are Zero-Shot Image Animators，2023年12月10日21:36:32
* DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing，2023年12月10日21:36:39
* Customizing Motion in Text-to-Video Diffusion Models，2023年12月11日13:59:22
* DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models，2023年12月11日13:59:32
* Video-Based Rendering Techniques: A Survey，2023年12月11日13:59:52
* Photorealistic Video Generation with Diffusion Models，2023年12月12日13:15:54
* Inferring Hybrid Neural Fluid Fields from Videos，2023年12月12日13:17:22
* A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing，2023年12月12日13:18:36
* MotionCrafter: One-Shot Motion Customization of Diffusion Models，2023年12月12日13:20:42
* FreeInit: Bridging Initialization Gap in Video Diffusion Models, 2023年12月13日12:41:44
* PEEKABOO: Interactive Video Generation via Masked-Diffusion，2023年12月13日12:42:07
* Neutral Editing Framework for Diffusion-based Video Editing，2023年12月13日12:43:59
* VideoLCM: Video Latent Consistency Model，2023年12月15日10:48:36
* Motion Flow Matching for Human Motion Synthesis and Editing，2023年12月15日10:49:07
* Neural Video Fields Editing，2023年12月15日10:50:26
* IQNet: Image Quality Assessment Guided Just Noticeable Difference Prefiltering For Versatile Video Coding，2023年12月18日20:13:47
* VidToMe: Video Token Merging for Zero-Shot Video Editing，2023年12月19日19:38:45
---

# Human-interaction
* Disentangled Interaction Representation for One-Stage Human-Object Interaction Detection
* HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models，2023年12月12日13:17:32
* Stellar: Systematic Evaluation of Human-Centric Personalized Text-to-Image Methods，2023年12月12日13:18:14
* LEMON: Learning 3D Human-Object Interaction Relation from 2D Images，2023年12月15日10:48:58
* Ins-HOI: Instance Aware Human-Object Interactions Recovery，2023年12月18日20:12:32
* Primitive-based 3D Human-Object Interaction Modelling and Programming，2023年12月19日19:38:19



# Human
*  HumanCoser: Layered 3D Human Generation via Semantic-Aware Diffusion Model，2023年12月12日13:20:05
*  Disentangled Representation Learning for Controllable Person Image Generation，2023年12月12日13:20:34
*  Towards Detailed Text-to-Motion Synthesis via Basic-to-Advance Hierarchical Diffusion Model,2023年12月19日19:37:45
*  MVHuman: Tailoring 2D Diffusion with Multi-view Sampling For Realistic 3D Human Generation，2023年12月19日19:39:13
*  HuTuMotion: Human-Tuned Navigation of Latent Motion Diffusion Models with Minimal Feedback，2023年12月20日12:25:36

