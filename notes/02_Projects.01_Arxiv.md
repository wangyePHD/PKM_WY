---
id: xutx0mtszok5w9q8n6d19xt
title: 01_Arxiv
desc: ''
updated: 1702274408672
created: 1699497182153
---


![图 0](assets/images/cc414f0cf3d4025af5df1a8f950208845b69deb84a4938c59342c746718d46d1.png)  


# **大模型**
## MLLM

* NExT-Chat: An LMM for Chat, Detection and Segmentation，2023年11月9日，https://arxiv.org/pdf/2311.04498.pdf
* OtterHD: A High-Resolution Multi-modality Model, 2023年11月8日，https://arxiv.org/pdf/2311.04219.pdf
* Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs，https://arxiv.org/pdf/2311.14656.pdf，2023年11月27日
* OneLLM: One Framework to Align All Modalities with Language，2023年12月7日12:37:54

## VLM
* Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks，2023年11月13日，https://arxiv.org/pdf/2311.06242.pdf

---


# Visual In-Context Learning
* Visual In-Context Prompting, https://arxiv.org/pdf/2311.13601.pdf, 2023年11月23日12:49:48
* IMPROV: INPAINTING-BASED MULTIMODAL PROMPTING FOR COMPUTER VISION TASKS，https://arxiv.org/pdf/2312.01771.pdf，2023年12月6日23:52:47
* Improving In-Context Learning in Diffusion Models with Visual Context-Modulated Prompts，2023年12月7日00:14:36，https://arxiv.org/abs/2312.01408
* Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning，2023年12月7日12:34:30


---

# Diffusion

## Control
* ToddlerDiffusion: Flash Interpretable Controllable Diffusion Model, https://arxiv.org/pdf/2311.14542.pdf, 2023年11月27日

## Efficient-Diffusion Model
* LCM-LORA: A UNIVERSAL STABLE-DIFFUSION
ACCELERATION MODULE, https://arxiv.org/pdf/2311.05556.pdf，2023年11月10日18:43:16
* WarpDiffusion: Efficient Diffusion Model for High-Fidelity Virtual Try-on, 2023年12月7日12:38:09
* SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation，2023年12月11日14:00:07
---

# **3D** 

## 3D-aware Generation

* WILDFUSION: LEARNING 3D-AWARE LATENT DIFFUSION MODELS IN VIEW SPACE, https://arxiv.org/pdf/2311.13570.pdf，2023年11月23日12:48:32
* Enhancing Diffusion Models with 3D Perspective Geometry Constraints，2023年12月7日00:30:50


## 纹理上色
* 3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models, https://arxiv.org/pdf/2311.05464.pdf，2023年11月10日18:43:19


## Gaussian Splatting
* GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting，https://arxiv.org/pdf/2311.14521.pdf，2023年11月27日
* GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis，2023年12月7日00:35:30


## Mesh
* Consistent Mesh Diffusion，2023年12月7日00:30:24

---


# Image 

## Style & Appearance
* ControlStyle: Text-Driven Stylized Image Generation Using Diffusion Priors，https://arxiv.org/pdf/2311.05463.pdf，2023年11月10日18:43:29
* ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation，https://arxiv.org/abs/2312.02109，2023年12月6日
* Style Aligned Image Generation via Shared Attention，2023年12月7日00:38:03


## Editing
* Inversion-Free Image Editing with Natural Language，2023年12月11日13:59:02

## Generation
* FoodFusion: A Latent Diffusion Model for Realistic Food Image Generation，2023年12月7日12:29:17
* KANDINSKY 3.0 TECHNICAL REPORT, 2023年12月7日12:31:51
* Self-conditioned Image Generation via Generating Representations，2023年12月7日12:37:27，**KAIMING**
* Context Diffusion: In-Context Aware Image Generation，2023年12月7日12:41:59

## Inpainting
* A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting，2023年12月7日12:40:45
* DreamInpainter: Text-Guided Subject-Driven Image Inpainting with Diffusion Models，2023年12月10日21:36:54

## Others
* Instance-guided Cartoon Editing with a Large-scale Dataset，https://arxiv.org/abs/2312.01943，2023年12月6日
* Diffusing Colors: Image Colorization with Text Guided Diffusion，2023年12月10日21:35:56
* Fine-Tuning InstructPix2Pix for Advanced Image Colorization,2023年12月11日13:58:25
* HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models，2023年12月11日13:58:39


## 定制化
* Customization Assistant for Text-to-image Generation，2023年12月7日12:33:43
* InstructBooth: Instruction-following Personalized Text-to-Image Generation，2023年12月7日12:34:14
---


# 3D Generation

## Text+X->3D
* Instant3D: Instant Text-to-3D Generation，https://arxiv.org/pdf/2311.08403.pdf，2023年11月16日
* Control3D: Towards Controllable Text-to-3D Generation，https://arxiv.org/pdf/2311.05461.pdf, 2023年11月10日18:43:41
* IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts，https://arxiv.org/pdf/2310.05375.pdf，2023.10.09
* Instant3D: Fast Text-to-3D with Sparse-view Generation and Large Reconstruction Model, 2023年11月13日，https://openreview.net/forum?id=2lDQLiH1W4（ICLR24 review 信息）
* ControlDreamer: Stylized 3D Generation with Multi-View ControlNet,https://arxiv.org/abs/2312.01129,2023年12月7日00:20:17
* Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors，2023年12月11日13:58:48

## Image->3D
* HyperDreamer: Hyper-Realistic 3D Content Generation and Editing from a Single Image，2023年12月10日21:35:03


## 单视角合成
* One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion, https://arxiv.org/pdf/2311.07885.pdf, 2023年11月16日
* Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to 3D
Prior with Progressive Learning, https://arxiv.org/pdf/2311.13617.pdf, 2023年11月27日
* MVControl: Adding Conditional Control to Multi-view Diffusion for
Controllable Text-to-3D Generation，https://arxiv.org/pdf/2311.14494.pdf，2023年11月27日
* ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models，https://arxiv.org/abs/2312.01305，2023年12月7日00:17:11
* Free3D: Consistent Novel View Synthesis without 3D Representation，2023年12月10日
* 
## 多视角重建
* DMV3D: DENOISING MULTI-VIEW DIFFUSION USING
3D LARGE RECONSTRUCTION MODEL, https://arxiv.org/pdf/2311.09217.pdf, 2023年11月16日
*  DreamComposer: Controllable 3D Object Generation via Multi-View Conditions, 2023年12月7日12:39:52


## Editing
* Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training，https://arxiv.org/abs/2312.01663，2023年12月6日23:58:57
* Mesh-Guided Neural Implicit Field Editing，2023年12月7日00:33:04

## NeRF
* SANeRF-HQ: Segment Anything for NeRF in High Quality，https://arxiv.org/pdf/2312.01531.pdf，2023年12月7日00:02:40
* NeRFiller: Completing Scenes via Generative 3D Inpainting，2023年12月10日
* 
*  

--- 

# 4D

## 4D Dynamic Scene
* Animate124: Animating One Image to 4D Dynamic Scene, https://arxiv.org/pdf/2311.14603.pdf, 2023年11月27日
* Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle，2023年12月7日12:32:26
* 

---

# Video
* Decouple Content and Motion for Conditional Image-to-Video Generation,https://arxiv.org/pdf/2311.14294.pdf, 2023年11月27日
* VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence, https://arxiv.org/pdf/2312.02087.pdf，2023年12月6日

* Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models， https://arxiv.org/pdf/2312.01409.pdf，2023年12月7日00:10:19
* QPoser: Quantized Explicit Pose Prior Modeling for Controllable Pose Generation，https://arxiv.org/abs/2312.01104，2023年12月7日00:23:32
* VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models，2023年12月7日00:31:15
* F3-Pruning: A Training-Free and Generalized Pruning Strategy towards Faster and Finer Text-to-Video Synthesis
* DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance，2023年12月7日12:33:57
* MotionCtrl: A Unified and Flexible Motion Controller for Video Generation，2023年12月7日12:38:35
* MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model, 2023年12月7日13:41:20
* GenDeF: Learning Generative Deformation Field for Video Generation, 2023年12月10日
* GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation，2023年12月10日
* RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models，2023年12月10日21:35:16
* Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation，2023年12月10日21:35:24
* DreamVideo: Composing Your Dream Videos with Customized Subject and Motion，2023年12月10日21:35:38
* Towards 4D Human Video Stylization，2023年12月10日21:36:05
* MTVG : Multi-text Video Generation with Text-to-Video Models，2023年12月10日21:36:12
* AVID: Any-Length Video Inpainting with Diffusion Model，2023年12月10日21:36:20
* AnimateZero: Video Diffusion Models are Zero-Shot Image Animators，2023年12月10日21:36:32
* DiffusionAtlas: High-Fidelity Consistent Diffusion Video Editing，2023年12月10日21:36:39
* Customizing Motion in Text-to-Video Diffusion Models，2023年12月11日13:59:22
* DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models，2023年12月11日13:59:32
* Video-Based Rendering Techniques: A Survey，2023年12月11日13:59:52
---

# Human-interaction
* Disentangled Interaction Representation for One-Stage Human-Object Interaction Detection
* 